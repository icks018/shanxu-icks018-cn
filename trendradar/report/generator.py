# coding=utf-8
"""
æŠ¥å‘Šç”Ÿæˆæ¨¡å—

æä¾›æŠ¥å‘Šæ•°æ®å‡†å¤‡å’Œ HTML ç”ŸæˆåŠŸèƒ½ï¼š
- prepare_report_data: å‡†å¤‡æŠ¥å‘Šæ•°æ®
- generate_html_report: ç”Ÿæˆ HTML æŠ¥å‘Š
"""

from pathlib import Path
from typing import Dict, List, Optional, Callable
import os
import logging

# å¯¼å…¥AIå¤„ç†å™¨
try:
    from ..ai.processor import AIProcessor
except ImportError:
    AIProcessor = None

logger = logging.getLogger(__name__)


def prepare_report_data(
    stats: List[Dict],
    failed_ids: Optional[List] = None,
    new_titles: Optional[Dict] = None,
    id_to_name: Optional[Dict] = None,
    mode: str = "daily",
    rank_threshold: int = 3,
    matches_word_groups_func: Optional[Callable] = None,
    load_frequency_words_func: Optional[Callable] = None,
    ai_config: Optional[Dict] = None,
) -> Dict:
    """
    å‡†å¤‡æŠ¥å‘Šæ•°æ®

    Args:
        stats: ç»Ÿè®¡ç»“æœåˆ—è¡¨
        failed_ids: å¤±è´¥çš„ ID åˆ—è¡¨
        new_titles: æ–°å¢æ ‡é¢˜
        id_to_name: ID åˆ°åç§°çš„æ˜ å°„
        mode: æŠ¥å‘Šæ¨¡å¼ (daily/incremental/current)
        rank_threshold: æ’åé˜ˆå€¼
        matches_word_groups_func: è¯ç»„åŒ¹é…å‡½æ•°
        load_frequency_words_func: åŠ è½½é¢‘ç‡è¯å‡½æ•°
        ai_config: AIå¤„ç†é…ç½®

    Returns:
        Dict: å‡†å¤‡å¥½çš„æŠ¥å‘Šæ•°æ®
    """
    # åˆå§‹åŒ–AIå¤„ç†å™¨
    ai_processor = None
    print(f"ğŸ” è°ƒè¯•ä¿¡æ¯: ai_config = {ai_config}")
    print(f"ğŸ” è°ƒè¯•ä¿¡æ¯: AIProcessor = {AIProcessor}")
    
    if ai_config and AIProcessor:
        try:
            print("ğŸ” å¼€å§‹åˆå§‹åŒ–AIå¤„ç†å™¨...")
            ai_processor = AIProcessor(ai_config)
            print(f"ğŸ” AIå¤„ç†å™¨åˆ›å»ºå®Œæˆï¼Œenabled = {ai_processor.enabled}")
            if ai_processor.enabled:
                print("âœ… AIæ™ºèƒ½å¤„ç†å·²å¯ç”¨")
                logger.info("AIæ™ºèƒ½å¤„ç†å·²å¯ç”¨")
            else:
                print("âŒ AIæ™ºèƒ½å¤„ç†æœªå¯ç”¨")
        except Exception as e:
            print(f"âŒ åˆå§‹åŒ–AIå¤„ç†å™¨å¤±è´¥: {e}")
            logger.error(f"åˆå§‹åŒ–AIå¤„ç†å™¨å¤±è´¥: {e}")
    else:
        print(f"âŒ AIé…ç½®æˆ–å¤„ç†å™¨ä¸å¯ç”¨: ai_config={bool(ai_config)}, AIProcessor={bool(AIProcessor)}")
    
    processed_new_titles = []

    # åœ¨å¢é‡æ¨¡å¼ä¸‹éšè—æ–°å¢æ–°é—»åŒºåŸŸ
    hide_new_section = mode == "incremental"

    # åªæœ‰åœ¨ééšè—æ¨¡å¼ä¸‹æ‰å¤„ç†æ–°å¢æ–°é—»éƒ¨åˆ†
    if not hide_new_section:
        filtered_new_titles = {}
        if new_titles and id_to_name:
            # å¦‚æœæä¾›äº†åŒ¹é…å‡½æ•°ï¼Œä½¿ç”¨å®ƒè¿‡æ»¤
            if matches_word_groups_func and load_frequency_words_func:
                word_groups, filter_words, global_filters = load_frequency_words_func()
                for source_id, titles_data in new_titles.items():
                    filtered_titles = {}
                    for title, title_data in titles_data.items():
                        if matches_word_groups_func(title, word_groups, filter_words, global_filters):
                            filtered_titles[title] = title_data
                    if filtered_titles:
                        filtered_new_titles[source_id] = filtered_titles
            else:
                # æ²¡æœ‰åŒ¹é…å‡½æ•°æ—¶ï¼Œä½¿ç”¨å…¨éƒ¨
                filtered_new_titles = new_titles

            # æ‰“å°è¿‡æ»¤åçš„æ–°å¢çƒ­ç‚¹æ•°ï¼ˆä¸æ¨é€æ˜¾ç¤ºä¸€è‡´ï¼‰
            original_new_count = sum(len(titles) for titles in new_titles.values()) if new_titles else 0
            filtered_new_count = sum(len(titles) for titles in filtered_new_titles.values()) if filtered_new_titles else 0
            if original_new_count > 0:
                print(f"é¢‘ç‡è¯è¿‡æ»¤åï¼š{filtered_new_count} æ¡æ–°å¢çƒ­ç‚¹åŒ¹é…ï¼ˆåŸå§‹ {original_new_count} æ¡ï¼‰")

        if filtered_new_titles and id_to_name:
            for source_id, titles_data in filtered_new_titles.items():
                source_name = id_to_name.get(source_id, source_id)
                source_titles = []

                for title, title_data in titles_data.items():
                    url = title_data.get("url", "")
                    mobile_url = title_data.get("mobileUrl", "")
                    ranks = title_data.get("ranks", [])

                    processed_title = {
                        "title": title,
                        "source_name": source_name,
                        "time_display": "",
                        "count": 1,
                        "ranks": ranks,
                        "rank_threshold": rank_threshold,
                        "url": url,
                        "mobile_url": mobile_url,
                        "is_new": True,
                    }
                    source_titles.append(processed_title)

                if source_titles:
                    processed_new_titles.append(
                        {
                            "source_id": source_id,
                            "source_name": source_name,
                            "titles": source_titles,
                        }
                    )

    processed_stats = []
    for stat in stats:
        if stat["count"] <= 0:
            continue

        processed_titles = []
        for title_data in stat["titles"]:
            processed_title = {
                "title": title_data["title"],
                "source_name": title_data["source_name"],
                "time_display": title_data["time_display"],
                "count": title_data["count"],
                "ranks": title_data["ranks"],
                "rank_threshold": title_data["rank_threshold"],
                "url": title_data.get("url", ""),
                "mobile_url": title_data.get("mobileUrl", ""),
                "is_new": title_data.get("is_new", False),
            }
            processed_titles.append(processed_title)

        processed_stats.append(
            {
                "word": stat["word"],
                "count": stat["count"],
                "percentage": stat.get("percentage", 0),
                "titles": processed_titles,
            }
        )

    # AIæ™ºèƒ½å¤„ç†
    if ai_processor and ai_processor.enabled:
        logger.info("å¼€å§‹AIæ™ºèƒ½å¤„ç†...")
        
        # æ”¶é›†æ‰€æœ‰æ–°é—»æ•°æ®è¿›è¡ŒAIå¤„ç†
        all_news_items = []
        
        # å¤„ç†ç»Ÿè®¡æ•°æ®ä¸­çš„æ–°é—»
        for stat in processed_stats:
            for title_data in stat["titles"]:
                news_item = {
                    "title": title_data["title"],
                    "content": title_data.get("content", ""),
                    "url": title_data.get("url", ""),
                    "source": title_data.get("source_name", ""),
                    "keyword": stat["word"]
                }
                all_news_items.append(news_item)
        
        # å¤„ç†æ–°å¢æ–°é—»
        for source in processed_new_titles:
            for title_data in source["titles"]:
                news_item = {
                    "title": title_data["title"],
                    "content": title_data.get("content", ""),
                    "url": title_data.get("url", ""),
                    "source": title_data.get("source_name", ""),
                    "keyword": "æ–°å¢çƒ­ç‚¹"
                }
                all_news_items.append(news_item)
        
        # æ‰§è¡ŒAIå¤„ç†
        if all_news_items:
            processed_news = ai_processor.process_news_list(all_news_items)
            categorized_news = ai_processor.categorize_news(processed_news)
            
            # å¦‚æœå¯ç”¨è§†é¢‘æ ¼å¼ï¼Œç”Ÿæˆæ ¼å¼åŒ–å†…å®¹
            if ai_processor.video_format:
                formatted_content = ai_processor.format_for_video(categorized_news)
                logger.info("AIå¤„ç†å®Œæˆï¼Œç”Ÿæˆè§†é¢‘å‹å¥½æ ¼å¼")
                
                # å°†AIå¤„ç†ç»“æœæ·»åŠ åˆ°è¿”å›æ•°æ®ä¸­
                return {
                    "stats": processed_stats,
                    "new_titles": processed_new_titles,
                    "failed_ids": failed_ids or [],
                    "total_new_count": sum(
                        len(source["titles"]) for source in processed_new_titles
                    ),
                    "ai_processed": True,
                    "ai_content": formatted_content,
                    "ai_categories": categorized_news,
                }

    return {
        "stats": processed_stats,
        "new_titles": processed_new_titles,
        "failed_ids": failed_ids or [],
        "total_new_count": sum(
            len(source["titles"]) for source in processed_new_titles
        ),
    }


def generate_html_report(
    stats: List[Dict],
    total_titles: int,
    failed_ids: Optional[List] = None,
    new_titles: Optional[Dict] = None,
    id_to_name: Optional[Dict] = None,
    mode: str = "daily",
    is_daily_summary: bool = False,
    update_info: Optional[Dict] = None,
    rank_threshold: int = 3,
    output_dir: str = "output",
    date_folder: str = "",
    time_filename: str = "",
    render_html_func: Optional[Callable] = None,
    matches_word_groups_func: Optional[Callable] = None,
    load_frequency_words_func: Optional[Callable] = None,
    enable_index_copy: bool = True,
    ai_config: Optional[Dict] = None,
) -> str:
    """
    ç”Ÿæˆ HTML æŠ¥å‘Š

    Args:
        stats: ç»Ÿè®¡ç»“æœåˆ—è¡¨
        total_titles: æ€»æ ‡é¢˜æ•°
        failed_ids: å¤±è´¥çš„ ID åˆ—è¡¨
        new_titles: æ–°å¢æ ‡é¢˜
        id_to_name: ID åˆ°åç§°çš„æ˜ å°„
        mode: æŠ¥å‘Šæ¨¡å¼ (daily/incremental/current)
        is_daily_summary: æ˜¯å¦æ˜¯æ¯æ—¥æ±‡æ€»
        update_info: æ›´æ–°ä¿¡æ¯
        rank_threshold: æ’åé˜ˆå€¼
        output_dir: è¾“å‡ºç›®å½•
        date_folder: æ—¥æœŸæ–‡ä»¶å¤¹åç§°
        time_filename: æ—¶é—´æ–‡ä»¶å
        render_html_func: HTML æ¸²æŸ“å‡½æ•°
        matches_word_groups_func: è¯ç»„åŒ¹é…å‡½æ•°
        load_frequency_words_func: åŠ è½½é¢‘ç‡è¯å‡½æ•°
        enable_index_copy: æ˜¯å¦å¤åˆ¶åˆ° index.html

    Returns:
        str: ç”Ÿæˆçš„ HTML æ–‡ä»¶è·¯å¾„
    """
    if is_daily_summary:
        if mode == "current":
            filename = "å½“å‰æ¦œå•æ±‡æ€».html"
        elif mode == "incremental":
            filename = "å½“æ—¥å¢é‡.html"
        else:
            filename = "å½“æ—¥æ±‡æ€».html"
    else:
        filename = f"{time_filename}.html"

    # æ„å»ºè¾“å‡ºè·¯å¾„
    output_path = Path(output_dir) / date_folder / "html"
    output_path.mkdir(parents=True, exist_ok=True)
    file_path = str(output_path / filename)

    # å‡†å¤‡æŠ¥å‘Šæ•°æ®
    report_data = prepare_report_data(
        stats,
        failed_ids,
        new_titles,
        id_to_name,
        mode,
        rank_threshold,
        matches_word_groups_func,
        load_frequency_words_func,
        ai_config,
    )

    # æ¸²æŸ“ HTML å†…å®¹
    if render_html_func:
        html_content = render_html_func(
            report_data, total_titles, is_daily_summary, mode, update_info
        )
    else:
        # é»˜è®¤ç®€å• HTML
        html_content = f"<html><body><h1>Report</h1><pre>{report_data}</pre></body></html>"

    # å†™å…¥æ–‡ä»¶
    with open(file_path, "w", encoding="utf-8") as f:
        f.write(html_content)

    # å¦‚æœæ˜¯æ¯æ—¥æ±‡æ€»ä¸”å¯ç”¨ index å¤åˆ¶
    if is_daily_summary and enable_index_copy:
        # ç”Ÿæˆåˆ°æ ¹ç›®å½•ï¼ˆä¾› GitHub Pages è®¿é—®ï¼‰
        root_index_path = Path("index.html")
        with open(root_index_path, "w", encoding="utf-8") as f:
            f.write(html_content)

        # åŒæ—¶ç”Ÿæˆåˆ° output ç›®å½•ï¼ˆä¾› Docker Volume æŒ‚è½½è®¿é—®ï¼‰
        output_index_path = Path(output_dir) / "index.html"
        Path(output_dir).mkdir(parents=True, exist_ok=True)
        with open(output_index_path, "w", encoding="utf-8") as f:
            f.write(html_content)

    return file_path
